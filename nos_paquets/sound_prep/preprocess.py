
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
import os
import IPython.display as ipd
import pandas as pd
from nos_paquets.sound_prep.params import *

### ------------ Etape 1: Definition des paramÃ¨tres ------------

class Conf:
    sampling_rate = SAMPLING_RATE  # FrÃ©quence d'Ã©chantillonnage (Hz) # on l'a fixÃ© Ã  16000
    duration = DURATION  # DurÃ©e cible en secondes
    n_mels = N_MELS  # Nombre de bandes de Mel

    hop_length = HOP_LENGTH  # DÃ©termine le nombre de frames temporelles
    fmin = FMIN  # FrÃ©quence minimale
    fmax = FMAX  # FrÃ©quence maximale (Nyquist)
    n_fft = N_FFT  # Taille de la FFT
    samples = SAMPLES  # Nombre total d'Ã©chantillons

conf = Conf()

### ------------ Etape 2: PremiÃ¨re fonction pour Lecture et nettoyage de l'audio ------------

def read_audio(conf, pathname, trim_long_data=True):

    print(f"ğŸ’â€‹ğŸ’â€‹ğŸ’â€‹ {pathname} ğŸ’â€‹ğŸ’â€‹ğŸ’â€‹")

    if "fma" not in pathname:
        y, sr = librosa.load(pathname,
                             sr=conf.sampling_rate,
                             duration=conf.duration,
                             mono=True)
        # Charge le fichier audio pathname avec une frÃ©quence d'Ã©chantillonnage dÃ©finie dans conf.sampling_rate (44100 Hz dans ce cas)
        # y est le signal audio sous forme d'un tableau NumPy
        # sr est la frÃ©quence d'Ã©chantillonnage

        if 0 < len(y): # Ã‰vite une erreur en cas de fichier audio vide
            y, _ = librosa.effects.trim(y) # Supprime les silences au dÃ©but et Ã  la fin
        # La fonction librosa.effects.trim(y) supprime les parties silencieuses en dÃ©but et fin d'audio

        # Si l'audio est plus long que la durÃ©e cible, on coupe l'excÃ©dent.
        # Si l'audio est trop court, on ajoute du padding (remplissage avec des zÃ©ros) des deux cÃ´tÃ©s pour uniformiser la taille.

        if len(y) > conf.samples: # Si l'audio est trop long
            if trim_long_data:
                y = y[0:0+conf.samples] # On garde seulement la partie nÃ©cessaire

        else: # Si l'audio est trop court, on ajoute du padding
            padding = conf.samples - len(y)    # Nombre d'Ã©chantillons Ã  ajouter
            offset = padding // 2 # Ajout Ã©quilibrÃ© Ã  gauche et Ã  droite
            y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')

        return y

    else: ## quand l'extrait provient de FMA, il fait 30sec : on le split en 3x10sec y_1, y_2, y_3
        print("ğŸ’â€‹1 splitğŸ’â€‹")
        y_1, sr_1 = librosa.load(pathname,
                                 sr=conf.sampling_rate,
                                 duration=conf.duration,
                                 mono=True,
                                 offset=0.0)
        if 0 < len(y_1): # Ã‰vite une erreur en cas de fichier audio vide
            y_1, _ = librosa.effects.trim(y_1)

        if len(y_1) > conf.samples: # Si l'audio est trop long
            if trim_long_data:
                y_1 = y_1[0:0+conf.samples]

        else: # Si l'audio est trop court, on ajoute du padding
            padding = conf.samples - len(y_1)
            offset = padding // 2
            y_1 = np.pad(y_1, (offset, conf.samples - len(y_1) - offset), 'constant')

        print("ğŸ’â€‹2 splitğŸ’â€‹")
        y_2, sr_1 = librosa.load(pathname,
                                 sr=conf.sampling_rate,
                                 duration=conf.duration,
                                 mono=True,
                                 offset=10.0)
        if 0 < len(y_2): # Ã‰vite une erreur en cas de fichier audio vide
            y_2, _ = librosa.effects.trim(y_2)

        if len(y_2) > conf.samples: # Si l'audio est trop long
            if trim_long_data:
                y_2 = y_2[0:0+conf.samples]

        else: # Si l'audio est trop court, on ajoute du padding
            padding = conf.samples - len(y_2)
            offset = padding // 2
            y_2 = np.pad(y_2, (offset, conf.samples - len(y_2) - offset), 'constant')

        print("ğŸ’â€‹3 splitğŸ’â€‹")
        y_3, sr_1 = librosa.load(pathname,
                                 sr=conf.sampling_rate,
                                 duration=conf.duration,
                                 mono=True,
                                 offset=20.0)
        if 0 < len(y_3): # Ã‰vite une erreur en cas de fichier audio vide
            y_3, _ = librosa.effects.trim(y_3)

        if len(y_3) > conf.samples: # Si l'audio est trop long
            if trim_long_data:
                y_3 = y_3[0:0+conf.samples]

        else: # Si l'audio est trop court, on ajoute du padding
            padding = conf.samples - len(y_3)
            offset = padding // 2
            y_3 = np.pad(y_3, (offset, conf.samples - len(y_3) - offset), 'constant')

        return (y_1, y_2, y_3)


### ------------ Etape 3: Conversion en spectrogramme ------------

def audio_to_melspectrogram(conf, audio):
    spectrogram = librosa.feature.melspectrogram(audio,
                                                 sr=conf.sampling_rate,
                                                 n_mels=conf.n_mels,
                                                 hop_length=conf.hop_length,
                                                 n_fft=conf.n_fft,
                                                 fmin=conf.fmin,
                                                 fmax=conf.fmax)
    spectrogram = librosa.power_to_db(spectrogram)
    spectrogram = spectrogram.astype(np.float32)
    return spectrogram    # type(spectrogram)=numpy.ndarray

    # Cette fonction gÃ©nÃ¨re un mel-spectrogram (une version transformÃ©e du spectrogramme classique) dans un premier temps
    # Puis conversion en dB (plus reprÃ©sentatif de la perception humaine du son)
    # Enfin, conversion en float32 pour optimiser la mÃ©moire et la compatibilitÃ©

### ------------ Etape 4: RÃ©union des deux fonctions ------------

def read_as_melspectrogram(conf, pathname, trim_long_data=False):

    try:
        x = read_audio(conf, pathname, trim_long_data)
        print(f"ğŸŒ·â€‹ file processed: {pathname} ğŸŒ·â€‹")

    except:
        print(f"âš ï¸ file ignored: {pathname} ğŸ’©â€‹ğŸ’©â€‹")


    if type(x)==tuple: # dans le cas oÃ¹ x est un tuple (x1, x2, X3), cad quand le fichier audio provient de fma
        prep_results_arr_1 = audio_to_melspectrogram(conf, x[0])
        prep_results_arr_2 = audio_to_melspectrogram(conf, x[1])
        prep_results_arr_3 = audio_to_melspectrogram(conf, x[2])

        return (prep_results_arr_1, prep_results_arr_2, prep_results_arr_3)

    else:
        prep_results_arr = audio_to_melspectrogram(conf, x)
        return prep_results_arr # type(spectrogram)=numpy.ndarray

# -> la fonction permet d'obtenir des np array
# exemple array_test = read_as_melspectrogram(conf,music_1_path,trim_long_data=False)

### ------------ Etape 5 (OPTIONNEL): Fonction pour plotter notre rÃ©sultat ressorti par l'Ã©tape 4 ------------

def plot_spectrogram(Y, sr, hop_length, y_axis="linear"):
    plt.figure(figsize=(25, 10))
    librosa.display.specshow(Y,
                             sr=sr,
                             hop_length=hop_length,
                             x_axis="time",
                             y_axis=y_axis)
    plt.colorbar(format="%+2.f")

# -> la fonction permet d'obtenir des plots
# exemple array_test = plot_spectrogram(array_test,sr=conf.sampling_rate,hop_length=conf.hop_length,y_axis="log")

### ------------ Etape 6 : Convertit le np array et l'ajoute Ã  un Dataframe ------------

def create_spectrogram_dataframe(conf, pathnames : list, trim_long_data=False):

    """cette fonction prend en entrÃ©e la liste des paths des fichiers audio et rend un dataframe:
    le dataframe contient :
                - l'id de la musique (plusieurs musique ont peut-Ãªtre la mÃªme id, notamment dans le cas de fma),
                - le nom du folder et des sous-folders dans lesquels le son est storÃ©,
                - l'array FLATTENED du spectrogramme,
                - la shape d'origine du spectrogramme pour pouvoir reshaper l'array avant de l'entrer dans les modÃ¨les,
                - une derniÃ¨re colonne "is_generated" : 1 si la musique est gÃ©nÃ©rÃ©e / 0 si la musique n'est pas gÃ©nÃ©rÃ©e
    """

    data = []

    count = 0
    for pathname in pathnames:

        print(f"number of files processed: {count}")
        count += 1

        music_id = pathname.split('/')[-1] # Extrait le nom de la musique
        folder_name = "/".join(pathname.split("/")[:-1]) # Extrait le lien / path
        prep_results_arr = read_as_melspectrogram(conf, pathname)  # retourne l'array du spec

        if type(prep_results_arr)==tuple: # si c'est un tuple, il faut faire 3 fois :
            for i in range(3):
                arr_shape = prep_results_arr[i].shape
                array_flatten = prep_results_arr[i].flatten()
                if "fake" in folder_name.lower():
                    is_generated=1
                else:
                    is_generated=0
                data.append([music_id, folder_name, array_flatten, arr_shape, is_generated])

        else:
            arr_shape = prep_results_arr.shape # pour garder la shape de l'array
            array_flatten = prep_results_arr.flatten() # on flatten l'array

            if "fake" in folder_name.lower():
                is_generated=1

            else:
                is_generated=0

            data.append([music_id, folder_name, array_flatten, arr_shape, is_generated])


    df = pd.DataFrame(data, columns=["music_id", "folder_name", "music_array", "shape_arr", "is_generated"])

    df["music_array"] = df["music_array"].apply(lambda x: x.tolist())

    print('â¤ï¸â€‹ğŸ©·â€‹ğŸ’›â€‹ğŸ’šâ€‹ğŸ’™â€‹ all data converted to df â¤ï¸â€‹ğŸ©·â€‹ğŸ’›â€‹ğŸ’šâ€‹ğŸ’™â€‹')

    return df

def create_csv(df):
    df.to_csv(PATH_PROCESSED_DATA,
              index=True)
    print('â¤ï¸â€‹ğŸ©·â€‹ğŸ’›â€‹ğŸ’šâ€‹ğŸ’™ all data saved as csv â¤ï¸â€‹ğŸ©·â€‹ğŸ’›â€‹ğŸ’šâ€‹ğŸ’™â€‹')
