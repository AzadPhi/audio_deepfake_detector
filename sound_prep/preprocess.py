
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt
import os
import IPython.display as ipd
import pandas as pd
from sound_prep.params import *

### ------------ Etape 1: Definition des paramÃ¨tres ------------

class Conf:
    sampling_rate = 16000  # FrÃ©quence d'Ã©chantillonnage (Hz) # on l'a fixÃ© Ã  16000
    duration = 10  # DurÃ©e cible en secondes
    hop_length = 347 * duration  # DÃ©termine le nombre de frames temporelles
    fmin = 20  # FrÃ©quence minimale
    fmax = sampling_rate // 2  # FrÃ©quence maximale (Nyquist)
    n_mels = 128  # Nombre de bandes de Mel
    n_fft = n_mels * 20  # Taille de la FFT
    samples = sampling_rate * duration  # Nombre total d'Ã©chantillons

conf = Conf()

### ------------ Etape 2: PremiÃ¨re fonction pour Lecture et nettoyage de l'audio ------------

def read_audio(conf, pathname, trim_long_data=True):
    y, sr = librosa.load(pathname, sr=conf.sampling_rate, mono=True)
    # Charge le fichier audio pathname avec une frÃ©quence d'Ã©chantillonnage dÃ©finie dans conf.sampling_rate (44100 Hz dans ce cas)
    # y est le signal audio sous forme d'un tableau NumPy
    # sr est la frÃ©quence d'Ã©chantillonnage

    if 0 < len(y): # Ã‰vite une erreur en cas de fichier audio vide
        y, _ = librosa.effects.trim(y) # Supprime les silences au dÃ©but et Ã  la fin
    # La fonction librosa.effects.trim(y) supprime les parties silencieuses en dÃ©but et fin d'audio

    # Si l'audio est plus long que la durÃ©e cible, on coupe l'excÃ©dent.
    # Si l'audio est trop court, on ajoute du padding (remplissage avec des zÃ©ros) des deux cÃ´tÃ©s pour uniformiser la taille.

    if len(y) > conf.samples: # Si l'audio est trop long
        if trim_long_data:
            y = y[0:0+conf.samples] # On garde seulement la partie nÃ©cessaire

    else: # Si l'audio est trop court, on ajoute du padding
        padding = conf.samples - len(y)    # Nombre d'Ã©chantillons Ã  ajouter
        offset = padding // 2 # Ajout Ã©quilibrÃ© Ã  gauche et Ã  droite
        y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')

    return y

### ------------ Etape 3: Conversion en spectrogramme ------------

def audio_to_melspectrogram(conf, audio):
    spectrogram = librosa.feature.melspectrogram(audio,
                                                 sr=conf.sampling_rate,
                                                 n_mels=conf.n_mels,
                                                 hop_length=conf.hop_length,
                                                 n_fft=conf.n_fft,
                                                 fmin=conf.fmin,
                                                 fmax=conf.fmax)
    spectrogram = librosa.power_to_db(spectrogram)
    spectrogram = spectrogram.astype(np.float32)
    return spectrogram

    # Cette fonction gÃ©nÃ¨re un mel-spectrogram (une version transformÃ©e du spectrogramme classique) dans un premier temps
    # Puis conversion en dB (plus reprÃ©sentatif de la perception humaine du son)
    # Enfin, conversion en float32 pour optimiser la mÃ©moire et la compatibilitÃ©

### ------------ Etape 4: RÃ©union des deux fonctions ------------

def read_as_melspectrogram(conf, pathname, trim_long_data=False):
    x = read_audio(conf, pathname, trim_long_data)
    prep_results_arr = audio_to_melspectrogram(conf, x)
    return prep_results_arr

# -> la fonction permet d'obtenir des np array
# exemple array_test = read_as_melspectrogram(conf,music_1_path,trim_long_data=False)

### ------------ Etape 5 (OPTIONNEL): Fonction pour plotter notre rÃ©sultat ressorti par l'Ã©tape 4 ------------

def plot_spectrogram(Y, sr, hop_length, y_axis="linear"):
    plt.figure(figsize=(25, 10))
    librosa.display.specshow(Y,
                             sr=sr,
                             hop_length=hop_length,
                             x_axis="time",
                             y_axis=y_axis)
    plt.colorbar(format="%+2.f")

# -> la fonction permet d'obtenir des plots
# exemple array_test = plot_spectrogram(array_test,sr=conf.sampling_rate,hop_length=conf.hop_length,y_axis="log")

### ------------ Etape 6 : Convertit le np array et l'ajoute Ã  un Dataframe ------------

def create_spectrogram_dataframe(conf, pathnames : list, trim_long_data=False):

    data = []

    for pathname in pathnames:
        music_id = pathname.split('/')[-1] # Extrait le nom de la musique
        folder_name = "/".join(pathname.split("/")[:-1]) # Extrait le lien / path
        prep_results_arr = read_as_melspectrogram(conf, pathname)  # IntÃ¨gre l'array

        if "fake" in folder_name.lower():
            is_generated=1

        else:
            is_generated=0

        data.append([music_id, folder_name, prep_results_arr, is_generated])


    df = pd.DataFrame(data, columns=["music_id", "folder_name", "music_array","is_generated"])
    print('â¤ï¸â€‹ğŸ©·â€‹ğŸ’›â€‹ğŸ’šâ€‹ğŸ’™â€‹ all data converted to df â¤ï¸â€‹ğŸ©·â€‹ğŸ’›â€‹ğŸ’šâ€‹ğŸ’™â€‹')
    return df

def create_csv(df):
    df.to_csv(f"/home/{os.environ.get('USER_NAME')}/audio_deepfake_detector/processed_data/music_preprocessed.csv",
              index=True)
    print('â¤ï¸â€‹ğŸ©·â€‹ğŸ’›â€‹ğŸ’šâ€‹ğŸ’™â€‹ all data saved as csv â¤ï¸â€‹ğŸ©·â€‹ğŸ’›â€‹ğŸ’šâ€‹ğŸ’™â€‹')

